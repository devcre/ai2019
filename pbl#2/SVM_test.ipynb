{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\dm2019\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "## loading mnist dataset\n",
    "\n",
    "raw_train = read_idx(\"./data/train-patterns-idx3-ubyte\")\n",
    "train_data = np.reshape(raw_train, (60000, 28*28))\n",
    "train_label = read_idx(\"./data/train-labels-idx1-ubyte\")\n",
    "\n",
    "raw_test = read_idx(\"./data/test-patterns-idx3-ubyte\")\n",
    "test_data = np.reshape(raw_test, (10000, 28*28))\n",
    "test_label = read_idx(\"./data/test-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train_p = train_data/255\n",
    "X_test_p = test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    def __init__(self, C=0.1, eta=0.001, batch_size=60, epochs=100, shuffle=True):\n",
    "        self.C = C\n",
    "        self.eta = eta\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.shuffle = shuffle\n",
    "        self.class_num = 0\n",
    "    \n",
    "    def fit(self, X, y, params=None, eval_score=None):\n",
    "        xn, xf = np.shape(X) # xn: data points of X, xf: number of feature\n",
    "        self.class_num = len(np.unique(y)) # 클래스 수 = 10\n",
    "        \n",
    "        y_ovr = self.encode_ovr(X, y)\n",
    "        \n",
    "        ## Initialize params: w, b\n",
    "        if params is None:\n",
    "            self.params = {\n",
    "                'w_': np.random.randn(xf, self.class_num), #(784,10) 정규분포난수\n",
    "                'b_': np.random.randn(1, self.class_num),\n",
    "                'aver_w': np.random.randn(xf, self.class_num),\n",
    "                'aver_b': np.random.randn(1, self.class_num)\n",
    "            }\n",
    "            w = self.params['w_']\n",
    "            b = self.params['b_']\n",
    "        \n",
    "        if eval_score is None:\n",
    "            self.score_val = 0\n",
    "            \n",
    "        cnt = 0 # SVM-SGD : k\n",
    "        \n",
    "        for epoch in range(0, self.epochs):\n",
    "            if self.shuffle:\n",
    "                X_shuffled, y_shuffled = self.shuffles(X, y_ovr)\n",
    "            \n",
    "            batch_count = int(xn/self.batch_size)\n",
    "                \n",
    "            for t in range(0, batch_count):\n",
    "                X_batch, y_batch, bs = self.mini_batch(X_shuffled, y_shuffled, t)\n",
    "                \n",
    "                X_batch = np.reshape(X_batch, (bs, xf))\n",
    "                y_batch = np.reshape(y_batch, (bs, self.class_num))\n",
    "                \n",
    "                loss = self.hinge_loss(X_batch, y_batch) # y*(X*W+b)\n",
    "                \n",
    "                loss = 1 - loss\n",
    "                loss[loss <= 0] = 0\n",
    "                loss[loss > 0] = 1\n",
    "                # loss = np.mean(loss)\n",
    "                \n",
    "                dw = np.zeros(self.params['w_'].shape)\n",
    "                db = np.zeros(self.params['b_'].shape)\n",
    "                \n",
    "                #fx = np.reshape(fx, (bs, self.class_num))\n",
    "                #tmp = np.multiply(y, fx)\n",
    "                #tmp = 1 - tmp\n",
    "                \n",
    "                #tmp[tmp <= 0] = 0\n",
    "                #tmp[tmp > 0] = 1\n",
    "                # y_tmp = np.multiply(y, loss.reshape(bs, self.class_num))\n",
    "                y_tmp = loss * y_batch\n",
    "                \n",
    "                dw = -(1/bs) * np.dot(np.transpose(y_tmp), X_batch) + np.transpose((1/self.C) * self.params['w_'])\n",
    "                db = -(1/bs) * np.sum(y_tmp, axis=0)\n",
    "                \n",
    "                self.params['w_'] = self.params['w_'] - np.transpose(self.eta * dw)\n",
    "                self.params['b_'] = self.params['b_'] - self.eta * db\n",
    "                \n",
    "                cnt += 1\n",
    "            \n",
    "            temp_w = cnt * (cnt/(cnt+1))*self.params['aver_w'] + (1/(cnt+1))*self.params['w_']\n",
    "            temp_b = cnt * (cnt/(cnt+1))*self.params['aver_b'] + (1/(cnt+1))*self.params['b_']\n",
    "            \n",
    "            prev_score = self.score_val\n",
    "            pres_score = self.score(X, y)\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"epochs: \", epoch)\n",
    "                print(\"prev_score: %d\", prev_score)\n",
    "                print(\"pres_score: %d\", pres_score)\n",
    "                print('\\n')\n",
    "            if prev_score < pres_score:\n",
    "                self.score_val = pres_score\n",
    "            \n",
    "            if self.det_weight(X, y, self.params['aver_w'], self.params['aver_b']) < self.det_weight(X, y, temp_w, temp_b):\n",
    "                self.params['aver_w'] = temp_w\n",
    "                self.params['aver_b'] = temp_b\n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    \n",
    "    def shuffles(self, X, y):\n",
    "        # Shuffle training data\n",
    "        ran = np.arange(0, np.shape(X)[0])\n",
    "        np.random.shuffle(ran)\n",
    "        return X[ran], y[ran]\n",
    "    \n",
    "    \n",
    "    def encode_ovr(self, X, y):\n",
    "        # 1로 이루어진 배열(np.shape(y)[0], class_num)\n",
    "        encode = np.ones((np.shape(y)[0], self.class_num))\n",
    "        # one-hot encode     \n",
    "        for i in range(self.class_num):\n",
    "            encode[:, i][y != i] = -1\n",
    "        return encode\n",
    "    \n",
    "    def mini_batch(self, X, y, t):\n",
    "        xn = np.shape(X)[0]\n",
    "        X_batch = X[t*self.batch_size : min(xn, (t+1)*self.batch_size)]\n",
    "        y_batch = y[t*self.batch_size : min(xn, (t+1)*self.batch_size)]\n",
    "        bs = min(xn, (t+1)*self.batch_size) - t*self.batch_size\n",
    "        \n",
    "        return X_batch, y_batch, bs\n",
    "    \n",
    "    def hinge_loss(self, X, y):\n",
    "        fx = np.dot(X, self.params['w_']) + self.params['b_'] # X*W+b\n",
    "        cfx = y * fx\n",
    "        \n",
    "        return cfx\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cla_score = np.dot(X, self.params['w_']) + self.params['b_']\n",
    "        pred = np.argmax(cla_score, axis=1)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        score = np.mean(pred == y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def det_weight(self, X, y, w1, b1):\n",
    "        temp = np.dot(X, w1) + b1\n",
    "        temp = temp.T\n",
    "        pred = np.argmax(temp, axis=1)\n",
    "        sco = np.mean(pred == y)\n",
    "        \n",
    "        return sco\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.params\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'C':self.C, 'batch_size':self.batch_size, 'epochs':self.epochs}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_p\n",
    "y = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0\n",
      "prev_score: %d 0\n",
      "pres_score: %d 0.7999333333333334\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\dm2019\\lib\\site-packages\\ipykernel_launcher.py:135: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  1\n",
      "prev_score: %d 0.7999333333333334\n",
      "pres_score: %d 0.8303666666666667\n",
      "\n",
      "\n",
      "epochs:  2\n",
      "prev_score: %d 0.8303666666666667\n",
      "pres_score: %d 0.8393333333333334\n",
      "\n",
      "\n",
      "epochs:  3\n",
      "prev_score: %d 0.8393333333333334\n",
      "pres_score: %d 0.8412833333333334\n",
      "\n",
      "\n",
      "epochs:  4\n",
      "prev_score: %d 0.8412833333333334\n",
      "pres_score: %d 0.83845\n",
      "\n",
      "\n",
      "epochs:  5\n",
      "prev_score: %d 0.8412833333333334\n",
      "pres_score: %d 0.8437\n",
      "\n",
      "\n",
      "epochs:  6\n",
      "prev_score: %d 0.8437\n",
      "pres_score: %d 0.8465666666666667\n",
      "\n",
      "\n",
      "epochs:  7\n",
      "prev_score: %d 0.8465666666666667\n",
      "pres_score: %d 0.8263833333333334\n",
      "\n",
      "\n",
      "epochs:  8\n",
      "prev_score: %d 0.8465666666666667\n",
      "pres_score: %d 0.8355\n",
      "\n",
      "\n",
      "epochs:  9\n",
      "prev_score: %d 0.8465666666666667\n",
      "pres_score: %d 0.8436333333333333\n",
      "\n",
      "\n",
      "epochs:  10\n",
      "prev_score: %d 0.8465666666666667\n",
      "pres_score: %d 0.84415\n",
      "\n",
      "\n",
      "epochs:  11\n",
      "prev_score: %d 0.8465666666666667\n",
      "pres_score: %d 0.8507333333333333\n",
      "\n",
      "\n",
      "epochs:  12\n",
      "prev_score: %d 0.8507333333333333\n",
      "pres_score: %d 0.8396833333333333\n",
      "\n",
      "\n",
      "epochs:  13\n",
      "prev_score: %d 0.8507333333333333\n",
      "pres_score: %d 0.8443666666666667\n",
      "\n",
      "\n",
      "epochs:  14\n",
      "prev_score: %d 0.8507333333333333\n",
      "pres_score: %d 0.8513666666666667\n",
      "\n",
      "\n",
      "epochs:  15\n",
      "prev_score: %d 0.8513666666666667\n",
      "pres_score: %d 0.8495\n",
      "\n",
      "\n",
      "epochs:  16\n",
      "prev_score: %d 0.8513666666666667\n",
      "pres_score: %d 0.85015\n",
      "\n",
      "\n",
      "epochs:  17\n",
      "prev_score: %d 0.8513666666666667\n",
      "pres_score: %d 0.8358333333333333\n",
      "\n",
      "\n",
      "epochs:  18\n",
      "prev_score: %d 0.8513666666666667\n",
      "pres_score: %d 0.8399833333333333\n",
      "\n",
      "\n",
      "epochs:  19\n",
      "prev_score: %d 0.8513666666666667\n",
      "pres_score: %d 0.8538\n",
      "\n",
      "\n",
      "epochs:  20\n",
      "prev_score: %d 0.8538\n",
      "pres_score: %d 0.855\n",
      "\n",
      "\n",
      "epochs:  21\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84855\n",
      "\n",
      "\n",
      "epochs:  22\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.85475\n",
      "\n",
      "\n",
      "epochs:  23\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8451666666666666\n",
      "\n",
      "\n",
      "epochs:  24\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8540166666666666\n",
      "\n",
      "\n",
      "epochs:  25\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8455333333333334\n",
      "\n",
      "\n",
      "epochs:  26\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8488666666666667\n",
      "\n",
      "\n",
      "epochs:  27\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8505666666666667\n",
      "\n",
      "\n",
      "epochs:  28\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.85025\n",
      "\n",
      "\n",
      "epochs:  29\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8486166666666667\n",
      "\n",
      "\n",
      "epochs:  30\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.83735\n",
      "\n",
      "\n",
      "epochs:  31\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8508\n",
      "\n",
      "\n",
      "epochs:  32\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8339833333333333\n",
      "\n",
      "\n",
      "epochs:  33\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8482833333333333\n",
      "\n",
      "\n",
      "epochs:  34\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8470666666666666\n",
      "\n",
      "\n",
      "epochs:  35\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8475\n",
      "\n",
      "\n",
      "epochs:  36\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8437166666666667\n",
      "\n",
      "\n",
      "epochs:  37\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8517833333333333\n",
      "\n",
      "\n",
      "epochs:  38\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84255\n",
      "\n",
      "\n",
      "epochs:  39\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8477\n",
      "\n",
      "\n",
      "epochs:  40\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8338166666666667\n",
      "\n",
      "\n",
      "epochs:  41\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8377666666666667\n",
      "\n",
      "\n",
      "epochs:  42\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84255\n",
      "\n",
      "\n",
      "epochs:  43\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.83645\n",
      "\n",
      "\n",
      "epochs:  44\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8308666666666666\n",
      "\n",
      "\n",
      "epochs:  45\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8493166666666667\n",
      "\n",
      "\n",
      "epochs:  46\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8412333333333334\n",
      "\n",
      "\n",
      "epochs:  47\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8516333333333334\n",
      "\n",
      "\n",
      "epochs:  48\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8486333333333334\n",
      "\n",
      "\n",
      "epochs:  49\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8514\n",
      "\n",
      "\n",
      "epochs:  50\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84475\n",
      "\n",
      "\n",
      "epochs:  51\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8525333333333334\n",
      "\n",
      "\n",
      "epochs:  52\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8461833333333333\n",
      "\n",
      "\n",
      "epochs:  53\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8406333333333333\n",
      "\n",
      "\n",
      "epochs:  54\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8412\n",
      "\n",
      "\n",
      "epochs:  55\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8449166666666666\n",
      "\n",
      "\n",
      "epochs:  56\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8472833333333334\n",
      "\n",
      "\n",
      "epochs:  57\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8369666666666666\n",
      "\n",
      "\n",
      "epochs:  58\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8395666666666667\n",
      "\n",
      "\n",
      "epochs:  59\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8527666666666667\n",
      "\n",
      "\n",
      "epochs:  60\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8489833333333333\n",
      "\n",
      "\n",
      "epochs:  61\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8458333333333333\n",
      "\n",
      "\n",
      "epochs:  62\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8284333333333334\n",
      "\n",
      "\n",
      "epochs:  63\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8442\n",
      "\n",
      "\n",
      "epochs:  64\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84305\n",
      "\n",
      "\n",
      "epochs:  65\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8503833333333334\n",
      "\n",
      "\n",
      "epochs:  66\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8425833333333334\n",
      "\n",
      "\n",
      "epochs:  67\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8093833333333333\n",
      "\n",
      "\n",
      "epochs:  68\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8466166666666667\n",
      "\n",
      "\n",
      "epochs:  69\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8469666666666666\n",
      "\n",
      "\n",
      "epochs:  70\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84585\n",
      "\n",
      "\n",
      "epochs:  71\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8391\n",
      "\n",
      "\n",
      "epochs:  72\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8476166666666667\n",
      "\n",
      "\n",
      "epochs:  73\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8359666666666666\n",
      "\n",
      "\n",
      "epochs:  74\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8443833333333334\n",
      "\n",
      "\n",
      "epochs:  75\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84895\n",
      "\n",
      "\n",
      "epochs:  76\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8362333333333334\n",
      "\n",
      "\n",
      "epochs:  77\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84565\n",
      "\n",
      "\n",
      "epochs:  78\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84835\n",
      "\n",
      "\n",
      "epochs:  79\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8443666666666667\n",
      "\n",
      "\n",
      "epochs:  80\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8464\n",
      "\n",
      "\n",
      "epochs:  81\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8454\n",
      "\n",
      "\n",
      "epochs:  82\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8541666666666666\n",
      "\n",
      "\n",
      "epochs:  83\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8433333333333334\n",
      "\n",
      "\n",
      "epochs:  84\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8452333333333333\n",
      "\n",
      "\n",
      "epochs:  85\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8443166666666667\n",
      "\n",
      "\n",
      "epochs:  86\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8510333333333333\n",
      "\n",
      "\n",
      "epochs:  87\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8448833333333333\n",
      "\n",
      "\n",
      "epochs:  88\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8482166666666666\n",
      "\n",
      "\n",
      "epochs:  89\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8404833333333334\n",
      "\n",
      "\n",
      "epochs:  90\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.85005\n",
      "\n",
      "\n",
      "epochs:  91\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8435166666666667\n",
      "\n",
      "\n",
      "epochs:  92\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8493166666666667\n",
      "\n",
      "\n",
      "epochs:  93\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8464166666666667\n",
      "\n",
      "\n",
      "epochs:  94\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8446333333333333\n",
      "\n",
      "\n",
      "epochs:  95\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8415666666666667\n",
      "\n",
      "\n",
      "epochs:  96\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8441333333333333\n",
      "\n",
      "\n",
      "epochs:  97\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8497333333333333\n",
      "\n",
      "\n",
      "epochs:  98\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.8464833333333334\n",
      "\n",
      "\n",
      "epochs:  99\n",
      "prev_score: %d 0.855\n",
      "pres_score: %d 0.84665\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w_': array([[-3.30266539e-14, -1.85420400e-13,  9.74531338e-14, ...,\n",
       "         -1.79271612e-13, -5.92273719e-14,  4.06681496e-14],\n",
       "        [-3.18109807e-14,  1.52173679e-13, -4.39750299e-14, ...,\n",
       "          1.32016269e-13,  4.49134548e-14,  4.88019159e-14],\n",
       "        [-5.94488962e-14,  7.74002823e-14, -3.45469468e-14, ...,\n",
       "         -6.25343844e-14, -2.69868342e-13, -1.56160405e-16],\n",
       "        ...,\n",
       "        [-8.49559620e-08,  7.63548824e-14, -1.18271532e-04, ...,\n",
       "         -2.64811659e-13,  1.19103449e-13,  1.36389548e-03],\n",
       "        [-7.98588340e-14,  1.06450386e-13, -2.98943295e-14, ...,\n",
       "         -5.17377507e-14, -2.98420804e-14,  7.88108935e-14],\n",
       "        [ 4.67203983e-14, -2.03942738e-14, -9.03796477e-15, ...,\n",
       "          2.97547251e-14,  6.48543544e-14,  2.83753821e-14]]),\n",
       " 'b_': array([[-2.88213985,  1.08406477, -1.73911725, -2.36571185, -0.88579247,\n",
       "         -0.19329255, -1.91554347, -0.37744413, -4.43350191, -2.24854788]]),\n",
       " 'aver_w': array([[-0.05768219,  1.13581065, -0.63024389, ...,  1.32192811,\n",
       "          1.67315224, -1.02346673],\n",
       "        [ 0.2211146 , -1.0113908 , -1.21694875, ..., -1.59757061,\n",
       "          0.20241327, -0.24821037],\n",
       "        [-1.48217543,  1.76104489,  0.1288    , ..., -0.22729662,\n",
       "          0.77711364, -2.40892746],\n",
       "        ...,\n",
       "        [ 0.09383906, -2.4217726 ,  1.09744224, ...,  1.4690488 ,\n",
       "         -0.81279445, -0.399151  ],\n",
       "        [ 0.31715748, -1.7794061 , -0.73594941, ..., -0.59463821,\n",
       "         -1.92141395, -0.631623  ],\n",
       "        [ 0.81329711, -0.36008875,  0.18231926, ..., -0.35491698,\n",
       "         -0.27881033,  0.45513574]]),\n",
       " 'aver_b': array([[-0.57256879,  0.22612765,  0.14613982,  0.71848549, -0.42467103,\n",
       "         -3.20070636, -1.11071666,  0.97074092,  0.00428647,  1.09945454]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVM(C=1000, eta=0.01, batch_size=20, epochs=200)\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 6 9 7]\n"
     ]
    }
   ],
   "source": [
    "pred = svm.predict(X_test_p)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_sco = accuracy_score(test_label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842\n"
     ]
    }
   ],
   "source": [
    "print(acc_sco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
